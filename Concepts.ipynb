{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'GrayScale_Image.PNG'/> <img src = 'RGB_Image.PNG'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two types of images as shown above. RGB which is colored images and GrayScale which is black and white images. In GrayScale images, the image will be of some n*n pixels where n can be any value (eg. 4 in the above image). And each of those pixels vary between 0 and 255. In and RGB image, the construct is same except it will have 3 channels, one each for red, green and blue (as shown in the image above). Since each pixel can vary between 0 and 255, each cell in an RGB image will be like [128,54,67] one representing red,green and blue respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is convolution operation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'Convolution_1.PNG'/> <img src = 'Convolution_2.PNG'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolution is a process by which we pass an image through a filter (eg. vertical edge filter in image 1 above) that gives an output which describes an aspect of the image (eg. vertical edges in the image). The output tell the info about the edges present in the image and where they are located. \n",
    "\n",
    "The formula of size of output is: n-f+1, where n is input image size, f is filter size\n",
    "\n",
    "The convolution operation takes place by sliding the filter over the image pixel, one stride at a time and calculates the output by doing summation of all multiplication of corresponding cells. The result is placed in the output cell. \n",
    "\n",
    "Note: The image is scaled before passing it to the convolution layer. \n",
    "\n",
    "As seen in the image 2 above, when we reverse scale the image with min and max, we can clearly see the edge partition between two colors. This gives the edge info from the input image. But since the input size is 6 and output size is 4, some information is lost. To avoid that, padding is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'Padding.PNG'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding is done to give protection against the loss of information in a convolution operation. n-f+1 formula leads to some loss of info in the output. Consider the image above, where n=6 and f=3 and output=4. To get the same output size as 6, we introduce a padding of value p=1 which provides a fence sort of structure around the input image (row at the top, row at the bottom, column to the right, column to the left). In this way when the filter is strided over the input, we get an output of size 6.\n",
    "\n",
    "The common ways of filling values in the padded cells are: 1. Zero padding (most common) 2. Neighbour padding (which takes the value of the neighbor cell closest to it)\n",
    "\n",
    "Zero padding is advantageous as it provides more edges in the image (as seen in the image above) so that the model could learn better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN vs ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'CNN vs ANN.PNG'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The operational flow of CNN is very much similar to that of an ANN. In ANN, for each neuron we do the operation of z=w(i)*x(i)+b followed by some activation function. In CNN, we do the convolution operation in a similar way. We slide the filter over the input and generate the edge info as output. Then the relu or some activation function is applied on the output cells. We can have any number of filters in a convolution layer (the number of filters is a hyperparameter). Also the number of convolution layers can also be stacked horizontally (no of layers to use is a hyperparameter). In the backpropagation, in ANN we update the weights, but here in CNN we update the values present inside the filters of the convolution layer. \n",
    "\n",
    "When we stack convolution layers horizontally, it behaves similar to that of a human brain, where the output of each layer is passed on to the next layer so that more details can be learned from the image using the previous outputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'Pooling.PNG'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pooling comes into the picture with relation to the term \"Location Invariant\" which says, as humans when we see an image and there are multiple faces in the image we automatically tend to detect it. In a similar way, the CNN model should also be able to detect the multiple faces in the image. That is where pooling comes in to the picture. \n",
    "\n",
    "Pooling takes the most impacting points in the image (as seen in the image above) so that multiple faces in the image can be detected by the model. In the above image as shown, for the pooling layer the stride is taken as 2 and instead of multiplying and summation, the max value in the block is taken as it is a max pooler. Similarly min and mean pooling can also be done and which provides the best result for the use case is taken forward. \n",
    "\n",
    "In backpropagation, the values in the pooling layer (filter?) is also updated along with the other filters and this can also be stacked horizontally in the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'Data Augmentation.PNG'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data augmentation is an important part in a CNN. Suppose we have only less number of images to pass as input to the model. That time data augmentation really helps out. We can do the different types of transformations as listed in the image above to create various variations of the image so that we can feed it to the model and increase our input size.\n",
    "\n",
    "Also this is needed in another perspective also. Suppose we give an image to the model to predict where the cat face is upside down. Then that time the model will struggle to predict the image. So as to the make the model robust, we do different types of data augmentations to provide various versions of the image to the model so that it can predict well on distorted or augmented images as well.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
